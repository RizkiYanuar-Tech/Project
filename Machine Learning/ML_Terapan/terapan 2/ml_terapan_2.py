# -*- coding: utf-8 -*-
"""ML_Terapan 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kzgy6vG3wzREVcopbIEglG8l64MvCooH
"""

!pip install numpy==1.24.3

!pip install scikit-surprise

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split
from surprise import Reader, Dataset, SVD
from surprise.model_selection import train_test_split
from surprise import accuracy

"""# **Data Gathering**"""

books = pd.read_csv("https://raw.githubusercontent.com/RizkiYanuar-Tech/ML_Terapan/refs/heads/main/terapan%202/books.csv")
ratings = pd.read_csv("https://raw.githubusercontent.com/RizkiYanuar-Tech/ML_Terapan/refs/heads/main/terapan%202/ratings.csv")
tags = pd.read_csv("https://raw.githubusercontent.com/RizkiYanuar-Tech/ML_Terapan/refs/heads/main/terapan%202/tags.csv")
book_tags = pd.read_csv("https://raw.githubusercontent.com/RizkiYanuar-Tech/ML_Terapan/refs/heads/main/terapan%202/book_tags.csv")

books.head()

ratings.head()

tags.head()

book_tags.head()

"""**Analisa**

Data books.csv berisikan metadata buku, ratings.csv berisikan rating user terhadap buku, tags.csv berisikan tags nama buku, book_tags.csv berisikan tag pada buku

# **Data Understanding**

## **Books.csv**
"""

books.info()

books.duplicated().sum()

plt.figure(figsize=(10, 6))
sns.boxplot(x='average_rating', data=books)
plt.title('Boxplot of Average Rating')
plt.show()

"""## **Rating.csv**"""

ratings.info()

ratings.duplicated().sum()

plt.figure(figsize=(10, 6))
sns.boxplot(x='rating', data=ratings)
plt.title('Boxplot of Rating')
plt.show()

"""## **Tags.csv**"""

tags.info()

tags.duplicated().sum()

"""## **Book_tags.csv**"""

book_tags.info()

book_tags.duplicated().sum()

"""**Analisa**

Pada data **books** tidak ditemukan adanya duplicate data, kemudian terdapat kolom yang tidak akan digunakan pada sistem rekomendasi yaitu kolom:
*   **id**
*   **best_book_id**
*   **isbn**
*   **isbn13**
*   **work_id**
*   **books_count**
*   **ratings_count**
*   **work_ratings_count**
*   **work_text_reviews_count**
*   **ratings_1**
*   **ratings_2**
*   **ratings_3**
*   **ratings_4**
*   **ratings_5**
*   **image_url**
*   **small_image_url**

ditemukan missing value dikolom:

*   isbn = 700 data
*   isbn13 = 585 data
*   original_publication_year = 21 data
*   original_title = 585 data
*   language_code = 1084 data

Pada data **books** ditemukan adanya **outlier** pada kolom average_rating, dimana ditemukan adanya nilai dibawah batas minimal (3.4) dan nilai diatas batas maksimal (4.5)

Pada data **ratings** tidak ditemukan missing value,sebaran data rating cenderung skewness kiri yang berarti sebagian besar pengguna memberi rating film 4 - 5, dan hanya sedikit yang memberi rating < 3, dan data ratings memiliki duplicated data

Pada data **tags** tidak ditemukan adanya data missing value dan duplicated

Sedangkan pada data **book_tags** ditemukan adanya data duplicated sebanyak 6 data, ditemukan juga nama kolom yang tidak konsisten untuk digabung berdasarkan book_id yang sama, dan terdapat kolom yang tidak akan digunakan yaitu kolom count

# **Data Preparation**

## **Handle Missing Value & Duplicated**

### **Books.csv**
"""

books.duplicated().sum()

books.dropna(axis=0, how='any', inplace=True)

books.info()

"""**Analisa**

Setelah dilakukan penghapusan data missing value pada data books, data yang asalnya 10K -> 7860 data

### **Ratings.csv**
"""

ratings.drop_duplicates(keep='first', inplace=True)

ratings.duplicated().sum()

"""**Analisa**

Pada data ratings dilakukan penghapusan terhadap 1644 data duplikat, dan menyimpan data pertama yang ditemukan

### **Book_tags.csv**
"""

book_tags.duplicated().sum()

book_tags.drop_duplicates(keep='first', inplace=True)

"""**Analisa**

Dilakukan penghapusan 6 data duplikat pada data book_tags, dan menyimpan data pertama yang ditemukan

## **Rename Columns**
"""

book_tags = book_tags.rename(columns={'goodreads_book_id': 'book_id'})

book_tags.info()

"""**Analisa**

Pada kolom **goodreads_book_id** dilakukan perubahan nama kolom menjadi **book_id** agar mudah digabungkan dengan table lain.

## **Outlier Handling**

Melakukan handling value pada data books kolom average rating
"""

Q1 = books['average_rating'].quantile(0.25)
Q3 = books['average_rating'].quantile(0.75)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

books['average_rating'] = np.where(books['average_rating'] < lower_bound, lower_bound, books['average_rating'])
books['average_rating'] = np.where(books['average_rating'] > upper_bound, upper_bound, books['average_rating'])

plt.figure(figsize=(10,5))
sns.boxplot(x='average_rating', data=books)
plt.title('Boxplot of Average Rating')
plt.show()

"""**Analisa**

Nilai average_rating diturunkan ke batas minimum dan maksimal menggunakan teknik capping, sekarang batas minimum menjadi sekitar 3.35 dan batas maksimal 4.6

## **Data Integration**
"""

book_tags = pd.merge(book_tags, tags, on='tag_id', how='inner')
books = pd.merge(books, ratings, on='book_id', how='inner')
books = pd.merge(books, book_tags, on='book_id', how='inner')

books.info()

"""**Analisa**

Dilakukan penggabungan data menjadi satu untuk menambah metadata yang dimiliki buku dan mendapatkan informasi history user berdasarkan rating

## **Drop Columns**
"""

books = books.drop(columns=['id', 'best_book_id', 'isbn', 'isbn13',
                            'image_url', 'work_id', 'small_image_url', 'original_title',
                            'books_count', 'ratings_count', 'work_ratings_count',
                            'work_text_reviews_count', 'ratings_1', 'ratings_2',
                            'ratings_3', 'ratings_4', 'ratings_5', 'tag_id','count'], axis=1)

books.info()

books.duplicated().sum()

books.drop_duplicates(keep='first', inplace=True)

"""**Analisa**

Dilakukan penghapusan pada kolom yang tidak digunakan pada sistem rekomendasi, dan melakukan penghapusan 100 data duplikat setelah dilakukan penggabungan

# **Modelling**

## **Splitting Dataset**
"""

# Ambil kolom untuk surprise
ratings_data = books[['user_id', 'book_id', 'rating']]

# Setup Reader
reader = Reader(rating_scale=(0.0, 5.0))
data = Dataset.load_from_df(ratings_data, reader)

# Split data
trainset, testset = train_test_split(data, test_size=0.2, random_state=42)

"""## **Collaborative Filtering**"""

def precision_recall_ndcg_at_k(predictions, k=10, threshold=4.0):
    top_n = get_top_n(predictions, n=k)

    precisions = []
    recalls = []
    ndcgs = []

    # Buat dictionary dari ground truth (rating aktual)
    truth = defaultdict(list)
    for uid, iid, true_r, est, _ in predictions:
        if true_r >= threshold:
            truth[uid].append(iid)

    for uid, user_predicted in top_n.items():
        pred_iids = [iid for iid, _ in user_predicted]
        true_iids = truth.get(uid, [])

        # Precision@K
        n_relevant = len(set(pred_iids) & set(true_iids))
        precisions.append(n_relevant / k)

        # Recall@K
        if true_iids:
            recalls.append(n_relevant / len(true_iids))
        else:
            recalls.append(0)

        # NDCG@K
        dcg = 0.0
        idcg = 0.0
        for i, iid in enumerate(pred_iids):
            if iid in true_iids:
                dcg += 1 / np.log2(i + 2)
        for i in range(min(len(true_iids), k)):
            idcg += 1 / np.log2(i + 2)
        ndcg = dcg / idcg if idcg > 0 else 0
        ndcgs.append(ndcg)

    print(f"Precision@{k}: {np.mean(precisions):.4f}")
    print(f"Recall@{k}: {np.mean(recalls):.4f}")
    print(f"NDCG@{k}: {np.mean(ndcgs):.4f}")

# Train model SVD
model = SVD()
model.fit(trainset)

# Evaluate
predictions = model.test(testset)
print("RMSE:", accuracy.rmse(predictions))

# Fungsi: Ambil Top-N item unik dari prediksi
def get_top_n(predictions, n=5):
    top_n = defaultdict(list)
    for uid, iid, true_r, est, _ in predictions:
        # Hindari duplikat item
        if iid not in [item[0] for item in top_n[uid]]:
            top_n[uid].append((iid, est))
    # Urutkan prediksi dari yang tertinggi, lalu ambil Top-N
    for uid, user_ratings in top_n.items():
        user_ratings.sort(key=lambda x: x[1], reverse=True)
        top_n[uid] = user_ratings[:n]
    return top_n

# Jalankan evaluasi
precision_recall_ndcg_at_k(predictions, k=5, threshold=4.0)

# Minta input user ID
target_user_id = int(input("Masukkan user ID: "))

# Ambil top-N rekomendasi
top_n = get_top_n(predictions, n=5)

# Tampilkan hanya untuk user ID yang diminta
if target_user_id in top_n:
    print(f"\nTop-5 Rekomendasi Buku untuk User {target_user_id}:")
    for (iid, est_rating) in top_n[target_user_id]:
        title = books.loc[books['book_id'] == iid, 'title'].values
        title_str = title[0] if len(title) > 0 else 'Unknown Title'
        print(f"  {title_str} (Predicted Rating: {est_rating:.2f})")
else:
    print(f"\nTidak ada rekomendasi untuk User ID {target_user_id}.")